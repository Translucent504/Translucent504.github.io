---
title: "[TIL] 29-Jan-2020"
excerpt_separator: "<!--more-->"
categories:
  - today-i-learned
tags:
  - til 
---

Data Storage, HDFS, ApacheSpark, Switching Fabric, RDD
<!--more-->

Data storage options:
 - SQL (Expensive, fast lookup, not scalable, changes in schema hard (require DDL))
 - NoSQL (Cheaper, slower lookup, linearly scalable)
 - ObjectStorage (Cheapest, slowest lookup, linearly scalable>

ApacheSpark is just a Java Virtual Machine (JVM) that take cares of parallelizing processing for us. The code we write is basically the driver JVM and then ApacheSpark communicates with smaller JVM executors. It does this by allocating compute/worker nodes which have their own JVM executors. It doesnt matter which core our driver JVM runs on since all of the compute clusters are on the network.

Switching Fabric:
 - A network topology that allows for High I/O Bandwidth
 - This is used for a system where we have networked file storage. The disks are not physically connected to any node and so are equally acessible to all nodes via networks.

Hadoop Data File System (HDFS):
 - This is used to store files that are too large to fit on one Hard disk.
 - We use this when we store data on JBOD (Just a bunch of Disks) or Directly attached storage instead of using some networked storage. Here our disks are physically attached to each node.
 - This is not posix compliant, so operating system filesystem cant directly communicate with it, instead a REST api is used or some other CLI applications.
 - Divide the file into equally sized chunks and distribute them on multiple hard drives.
 - Hadoop provides a virtual file interface which allows us to interact with the file as if it was a single large file rather than distributed.
 - since Hadoop keeps track of which drive which chunk of the file is on, we can utilize parallel processing by allocating the cpu that is closest to the drive with data that needs to be processed.

Resilient Distributed Dataset (RDD):
 - This is the basic primitive/first class citizen used by ApacheSpark.
 - They can be made from any underlying data system like SQl/NoSQL/ObjectStore etc.
 - They can be typed (int, str etc)
 - They are lazy (node only used if data is needed)
 - RDD is basically a bunch of data that is in the memory of our jvm worker nodes. When the sum of all our worker node's memory is filled up this is spilled to disk.
